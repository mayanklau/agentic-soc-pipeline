"""
Malware Agent - Specialized malware analysis and detection.

The malware agent is responsible for:
- Static analysis of suspicious files and binaries
- Dynamic analysis coordination (sandbox integration)
- YARA rule matching and generation
- Malware family classification
- Behavioral pattern analysis
"""

from __future__ import annotations

import base64
import hashlib
import json
import re
from typing import Any
from uuid import UUID

import structlog

from src.agents.base import AgentContext, AgentResult, BaseAgent
from src.models import IOC, MITREAttack

logger = structlog.get_logger()


class MalwareAgent(BaseAgent):
    """
    Malware analysis agent using CodeLlama for code understanding.
    
    Specializes in analyzing suspicious code, scripts, and binaries
    to identify malicious behavior and techniques.
    """
    
    agent_type = "malware"
    agent_description = "Malware Analysis Specialist"
    default_model = "codellama:7b"
    default_temperature = 0.1
    default_max_tokens = 2048
    
    # Known malicious patterns
    SUSPICIOUS_PATTERNS = {
        "powershell_encoded": r"-[Ee]nc(oded)?[Cc]ommand",
        "powershell_bypass": r"-[Ee]xecution[Pp]olicy\s+[Bb]ypass",
        "powershell_hidden": r"-[Ww]indow[Ss]tyle\s+[Hh]idden",
        "base64_blob": r"[A-Za-z0-9+/]{50,}={0,2}",
        "invoke_expression": r"[Ii]nvoke-[Ee]xpression|[Ii][Ee][Xx]",
        "download_cradle": r"(Net\.WebClient|Invoke-WebRequest|wget|curl).*http",
        "reflection_load": r"\[System\.Reflection\.Assembly\]::Load",
        "wmi_process": r"Win32_Process.*Create",
        "reg_run_key": r"(HKLM|HKCU).*\\Run",
        "scheduled_task": r"schtasks.*\/create",
        "service_create": r"sc\s+(create|config)",
        "mimikatz_pattern": r"(sekurlsa|kerberos|lsadump)::",
        "cobalt_strike": r"(beacon|sleeptime|jitter|spawnto)",
    }
    
    # MITRE mappings for common techniques
    TECHNIQUE_MAPPINGS = {
        "powershell_encoded": MITREAttack(
            tactic_id="TA0002", tactic_name="Execution",
            technique_id="T1059.001", technique_name="PowerShell"
        ),
        "download_cradle": MITREAttack(
            tactic_id="TA0011", tactic_name="Command and Control",
            technique_id="T1105", technique_name="Ingress Tool Transfer"
        ),
        "reflection_load": MITREAttack(
            tactic_id="TA0005", tactic_name="Defense Evasion",
            technique_id="T1620", technique_name="Reflective Code Loading"
        ),
        "reg_run_key": MITREAttack(
            tactic_id="TA0003", tactic_name="Persistence",
            technique_id="T1547.001", technique_name="Registry Run Keys"
        ),
        "scheduled_task": MITREAttack(
            tactic_id="TA0003", tactic_name="Persistence",
            technique_id="T1053.005", technique_name="Scheduled Task"
        ),
        "mimikatz_pattern": MITREAttack(
            tactic_id="TA0006", tactic_name="Credential Access",
            technique_id="T1003", technique_name="OS Credential Dumping"
        ),
    }
    
    async def _execute(self, context: AgentContext) -> AgentResult:
        """Execute malware analysis task."""
        task = context.task
        task_type = task.task_type
        
        if task_type == "analyze_artifact":
            return await self._analyze_artifact(context)
        elif task_type == "analyze_command_line":
            return await self._analyze_command_line(context)
        elif task_type == "classify_malware":
            return await self._classify_malware(context)
        elif task_type == "generate_yara":
            return await self._generate_yara_rule(context)
        elif task_type == "analyze_script":
            return await self._analyze_script(context)
        else:
            return AgentResult(
                task_id=task.task_id,
                agent_type=self.agent_type,
                success=False,
                error=f"Unknown task type: {task_type}",
            )
    
    async def _analyze_artifact(self, context: AgentContext) -> AgentResult:
        """
        Comprehensive analysis of a suspicious artifact.
        
        Combines static analysis, pattern matching, and LLM-based
        behavioral analysis.
        """
        alert_data = context.task.payload.get("alert", {})
        artifact_data = context.task.payload.get("artifact", {})
        
        # Extract artifact details
        file_hash = artifact_data.get("hash") or alert_data.get("process_hash")
        file_name = artifact_data.get("name") or alert_data.get("file_name")
        command_line = alert_data.get("command_line", "")
        
        analysis_results = {
            "hash_analysis": None,
            "pattern_analysis": None,
            "behavioral_analysis": None,
            "iocs_extracted": [],
            "mitre_techniques": [],
        }
        
        # 1. Pattern matching on command line
        if command_line:
            pattern_results = self._analyze_patterns(command_line)
            analysis_results["pattern_analysis"] = pattern_results
            analysis_results["mitre_techniques"].extend(pattern_results.get("mitre_techniques", []))
        
        # 2. LLM-based behavioral analysis
        if command_line or artifact_data.get("content"):
            behavioral = await self._llm_behavioral_analysis(
                command_line=command_line,
                content=artifact_data.get("content"),
                context=context,
            )
            analysis_results["behavioral_analysis"] = behavioral
        
        # 3. Check hash against known malware (would integrate with threat intel)
        if file_hash:
            analysis_results["hash_analysis"] = {
                "hash": file_hash,
                "hash_type": self._detect_hash_type(file_hash),
                "needs_threat_intel_lookup": True,
            }
            analysis_results["iocs_extracted"].append(
                IOC(
                    type="hash",
                    value=file_hash,
                    confidence=0.7,
                    source="malware_agent",
                    first_seen=context.task.assigned_at,
                    last_seen=context.task.assigned_at,
                ).model_dump()
            )
        
        # Determine overall verdict
        verdict = self._determine_verdict(analysis_results)
        
        # Build follow-up tasks
        follow_up_tasks = []
        if verdict["is_malicious"] and verdict["confidence"] >= 0.8:
            follow_up_tasks.append({
                "agent_type": "response",
                "task_type": "recommend_actions",
                "priority": 1,
                "payload": {
                    "alert": alert_data,
                    "malware_analysis": analysis_results,
                    "verdict": verdict,
                },
            })
        
        if verdict.get("needs_sandbox"):
            follow_up_tasks.append({
                "agent_type": "malware",
                "task_type": "sandbox_analysis",
                "priority": 2,
                "payload": {"artifact": artifact_data},
            })
        
        return AgentResult(
            task_id=context.task.task_id,
            agent_type=self.agent_type,
            success=True,
            confidence=verdict["confidence"],
            result={
                "analysis": analysis_results,
                "verdict": verdict,
            },
            reasoning=verdict["reasoning"],
            recommended_actions=verdict.get("recommended_actions", []),
            follow_up_tasks=follow_up_tasks,
        )
    
    async def _analyze_command_line(self, context: AgentContext) -> AgentResult:
        """
        Specialized analysis of suspicious command lines.
        
        Focuses on identifying malicious patterns in process command lines.
        """
        command_line = context.task.payload.get("command_line", "")
        
        if not command_line:
            return AgentResult(
                task_id=context.task.task_id,
                agent_type=self.agent_type,
                success=False,
                error="No command line provided",
            )
        
        # Pattern analysis
        pattern_results = self._analyze_patterns(command_line)
        
        # Decode encoded content if present
        decoded_content = self._decode_encoded_content(command_line)
        
        # LLM analysis
        system_prompt = """You are a malware analyst specializing in command line analysis.
Analyze command lines for malicious behavior, obfuscation, and attack techniques."""

        prompt = f"""Analyze this command line for malicious behavior:

COMMAND LINE:
{command_line}

{f"DECODED CONTENT:{chr(10)}{decoded_content}" if decoded_content else ""}

Pattern matches found: {json.dumps(pattern_results.get('matches', []))}

Provide analysis in JSON:
{{
    "is_malicious": true/false,
    "confidence": 0.0-1.0,
    "malicious_indicators": ["indicator1", "indicator2"],
    "obfuscation_techniques": ["technique1"],
    "likely_purpose": "description of what this command does",
    "mitre_techniques": ["T1234: Technique Name"],
    "severity": "low|medium|high|critical"
}}"""

        try:
            response = await self.llm_completion(
                prompt=prompt,
                system_prompt=system_prompt,
                json_mode=True,
            )
            
            llm_analysis = json.loads(response)
            
            # Combine pattern and LLM results
            combined_confidence = max(
                pattern_results.get("confidence", 0),
                llm_analysis.get("confidence", 0)
            )
            
            return AgentResult(
                task_id=context.task.task_id,
                agent_type=self.agent_type,
                success=True,
                confidence=combined_confidence,
                result={
                    "command_line": command_line,
                    "decoded_content": decoded_content,
                    "pattern_analysis": pattern_results,
                    "llm_analysis": llm_analysis,
                    "is_malicious": llm_analysis.get("is_malicious", False),
                },
                reasoning=llm_analysis.get("likely_purpose"),
                recommended_actions=(
                    ["Block process", "Isolate endpoint", "Collect forensic data"]
                    if llm_analysis.get("is_malicious")
                    else ["Continue monitoring"]
                ),
            )
            
        except Exception as e:
            # Fall back to pattern-only analysis
            return AgentResult(
                task_id=context.task.task_id,
                agent_type=self.agent_type,
                success=True,
                confidence=pattern_results.get("confidence", 0.5),
                result={
                    "command_line": command_line,
                    "pattern_analysis": pattern_results,
                    "method": "pattern_only_fallback",
                },
                reasoning=f"Pattern analysis: {len(pattern_results.get('matches', []))} suspicious patterns found",
            )
    
    async def _analyze_script(self, context: AgentContext) -> AgentResult:
        """
        Analyze suspicious scripts (PowerShell, VBScript, JavaScript, etc.).
        """
        script_content = context.task.payload.get("content", "")
        script_type = context.task.payload.get("type", "unknown")
        
        if not script_content:
            return AgentResult(
                task_id=context.task.task_id,
                agent_type=self.agent_type,
                success=False,
                error="No script content provided",
            )
        
        # Truncate very long scripts for LLM
        truncated = script_content[:8000] if len(script_content) > 8000 else script_content
        
        system_prompt = f"""You are an expert malware analyst specializing in {script_type} script analysis.
Identify malicious code patterns, obfuscation, and potential payloads."""

        prompt = f"""Analyze this {script_type} script for malicious behavior:

```
{truncated}
```

{"[TRUNCATED - showing first 8000 chars]" if len(script_content) > 8000 else ""}

Provide analysis in JSON:
{{
    "is_malicious": true/false,
    "confidence": 0.0-1.0,
    "malware_family": "family name if identifiable",
    "obfuscation_level": "none|light|moderate|heavy",
    "capabilities": ["capability1", "capability2"],
    "iocs": [
        {{"type": "domain|ip|url|hash", "value": "..."}}
    ],
    "mitre_techniques": ["T1234: Technique Name"],
    "deobfuscated_summary": "what the script actually does",
    "severity": "low|medium|high|critical"
}}"""

        try:
            response = await self.llm_completion(
                prompt=prompt,
                system_prompt=system_prompt,
                json_mode=True,
                max_tokens=4096,
            )
            
            analysis = json.loads(response)
            
            # Extract IOCs
            iocs = []
            for ioc_data in analysis.get("iocs", []):
                iocs.append(IOC(
                    type=ioc_data.get("type", "unknown"),
                    value=ioc_data.get("value", ""),
                    confidence=analysis.get("confidence", 0.5),
                    source="malware_agent_script_analysis",
                    first_seen=context.task.assigned_at,
                    last_seen=context.task.assigned_at,
                ))
            
            return AgentResult(
                task_id=context.task.task_id,
                agent_type=self.agent_type,
                success=True,
                confidence=analysis.get("confidence", 0.7),
                result={
                    "script_type": script_type,
                    "analysis": analysis,
                    "iocs_extracted": [i.model_dump() for i in iocs],
                },
                reasoning=analysis.get("deobfuscated_summary"),
                recommended_actions=(
                    ["Quarantine script", "Block execution", "Hunt for similar scripts"]
                    if analysis.get("is_malicious")
                    else ["Continue monitoring"]
                ),
            )
            
        except Exception as e:
            return AgentResult(
                task_id=context.task.task_id,
                agent_type=self.agent_type,
                success=False,
                error=str(e),
            )
    
    async def _classify_malware(self, context: AgentContext) -> AgentResult:
        """
        Classify malware into known families based on behavior and indicators.
        """
        indicators = context.task.payload.get("indicators", [])
        behaviors = context.task.payload.get("behaviors", [])
        
        system_prompt = """You are a malware classification expert.
Based on indicators and behaviors, identify the likely malware family."""

        prompt = f"""Classify this malware based on the following indicators and behaviors:

INDICATORS:
{json.dumps(indicators, indent=2)}

BEHAVIORS:
{json.dumps(behaviors, indent=2)}

Provide classification in JSON:
{{
    "primary_classification": "malware family name",
    "confidence": 0.0-1.0,
    "alternative_classifications": ["family1", "family2"],
    "malware_type": "ransomware|trojan|worm|rat|apt|etc",
    "known_threat_actors": ["actor1"],
    "similar_samples": ["hash1", "hash2"],
    "reasoning": "why this classification"
}}"""

        try:
            response = await self.llm_completion(
                prompt=prompt,
                system_prompt=system_prompt,
                json_mode=True,
            )
            
            classification = json.loads(response)
            
            return AgentResult(
                task_id=context.task.task_id,
                agent_type=self.agent_type,
                success=True,
                confidence=classification.get("confidence", 0.6),
                result=classification,
                reasoning=classification.get("reasoning"),
            )
            
        except Exception as e:
            return AgentResult(
                task_id=context.task.task_id,
                agent_type=self.agent_type,
                success=False,
                error=str(e),
            )
    
    async def _generate_yara_rule(self, context: AgentContext) -> AgentResult:
        """
        Generate a YARA rule based on malware analysis findings.
        """
        analysis = context.task.payload.get("analysis", {})
        sample_name = context.task.payload.get("sample_name", "unknown_malware")
        
        system_prompt = """You are a YARA rule expert.
Generate precise YARA rules that balance detection accuracy with false positive avoidance."""

        prompt = f"""Generate a YARA rule for detecting this malware:

SAMPLE NAME: {sample_name}

ANALYSIS FINDINGS:
{json.dumps(analysis, indent=2)}

Generate a complete YARA rule. Include:
- Appropriate meta information
- String patterns (both ascii and wide where appropriate)
- Condition logic
- Comments explaining the rule

Respond with ONLY the YARA rule, no additional explanation."""

        try:
            response = await self.llm_completion(
                prompt=prompt,
                system_prompt=system_prompt,
                json_mode=False,
                max_tokens=2048,
            )
            
            # Basic validation of YARA syntax
            is_valid = "rule " in response and "condition:" in response
            
            return AgentResult(
                task_id=context.task.task_id,
                agent_type=self.agent_type,
                success=is_valid,
                confidence=0.7 if is_valid else 0.3,
                result={
                    "yara_rule": response,
                    "sample_name": sample_name,
                    "syntax_valid": is_valid,
                },
                reasoning="Generated YARA rule based on analysis findings",
                recommended_actions=["Review rule before deployment", "Test against sample corpus"],
            )
            
        except Exception as e:
            return AgentResult(
                task_id=context.task.task_id,
                agent_type=self.agent_type,
                success=False,
                error=str(e),
            )
    
    def _analyze_patterns(self, content: str) -> dict[str, Any]:
        """Analyze content for suspicious patterns."""
        matches = []
        mitre_techniques = []
        
        for pattern_name, pattern in self.SUSPICIOUS_PATTERNS.items():
            if re.search(pattern, content, re.IGNORECASE):
                matches.append({
                    "pattern": pattern_name,
                    "description": pattern_name.replace("_", " ").title(),
                })
                
                if pattern_name in self.TECHNIQUE_MAPPINGS:
                    mitre_techniques.append(
                        self.TECHNIQUE_MAPPINGS[pattern_name].model_dump()
                    )
        
        # Calculate confidence based on number and severity of matches
        confidence = min(0.3 + (len(matches) * 0.15), 0.95)
        
        return {
            "matches": matches,
            "match_count": len(matches),
            "confidence": confidence,
            "mitre_techniques": mitre_techniques,
        }
    
    def _decode_encoded_content(self, command_line: str) -> str | None:
        """Attempt to decode base64 encoded content in command lines."""
        # Look for base64 patterns
        base64_pattern = r'[A-Za-z0-9+/]{40,}={0,2}'
        matches = re.findall(base64_pattern, command_line)
        
        decoded_parts = []
        for match in matches:
            try:
                decoded = base64.b64decode(match).decode('utf-16-le', errors='ignore')
                if decoded and len(decoded) > 10:
                    decoded_parts.append(decoded)
            except Exception:
                try:
                    decoded = base64.b64decode(match).decode('utf-8', errors='ignore')
                    if decoded and len(decoded) > 10:
                        decoded_parts.append(decoded)
                except Exception:
                    pass
        
        return "\n---\n".join(decoded_parts) if decoded_parts else None
    
    def _detect_hash_type(self, hash_value: str) -> str:
        """Detect the type of hash based on length."""
        length = len(hash_value)
        if length == 32:
            return "md5"
        elif length == 40:
            return "sha1"
        elif length == 64:
            return "sha256"
        elif length == 128:
            return "sha512"
        else:
            return "unknown"
    
    async def _llm_behavioral_analysis(
        self,
        command_line: str | None,
        content: str | None,
        context: AgentContext,
    ) -> dict[str, Any]:
        """Use LLM for behavioral analysis."""
        system_prompt = """You are a malware behavioral analyst.
Analyze the provided content to identify malicious behaviors and techniques."""

        analysis_target = command_line or content or ""
        if len(analysis_target) > 4000:
            analysis_target = analysis_target[:4000] + "\n[TRUNCATED]"
        
        prompt = f"""Analyze this for malicious behavior:

{analysis_target}

Provide behavioral analysis in JSON:
{{
    "behaviors_identified": ["behavior1", "behavior2"],
    "risk_level": "low|medium|high|critical",
    "persistence_mechanisms": ["mechanism1"],
    "evasion_techniques": ["technique1"],
    "data_exfiltration_risk": true/false,
    "lateral_movement_risk": true/false,
    "summary": "brief summary of findings"
}}"""

        try:
            response = await self.llm_completion(
                prompt=prompt,
                system_prompt=system_prompt,
                json_mode=True,
            )
            return json.loads(response)
        except Exception:
            return {"error": "behavioral analysis failed"}
    
    def _determine_verdict(self, analysis_results: dict) -> dict[str, Any]:
        """Determine overall malware verdict from analysis results."""
        is_malicious = False
        confidence = 0.0
        reasons = []
        
        # Check pattern analysis
        pattern = analysis_results.get("pattern_analysis", {})
        if pattern.get("match_count", 0) >= 2:
            is_malicious = True
            confidence = max(confidence, pattern.get("confidence", 0))
            reasons.append(f"{pattern.get('match_count')} suspicious patterns detected")
        
        # Check behavioral analysis
        behavioral = analysis_results.get("behavioral_analysis", {})
        if behavioral.get("risk_level") in ["high", "critical"]:
            is_malicious = True
            confidence = max(confidence, 0.8)
            reasons.append(f"High-risk behaviors: {behavioral.get('risk_level')}")
        
        if behavioral.get("data_exfiltration_risk") or behavioral.get("lateral_movement_risk"):
            is_malicious = True
            confidence = max(confidence, 0.75)
            reasons.append("Data exfiltration or lateral movement indicators")
        
        # Determine if sandbox analysis is needed
        needs_sandbox = (
            is_malicious and confidence < 0.9
        ) or (
            pattern.get("match_count", 0) >= 1 and not is_malicious
        )
        
        return {
            "is_malicious": is_malicious,
            "confidence": confidence,
            "reasoning": "; ".join(reasons) if reasons else "No malicious indicators found",
            "needs_sandbox": needs_sandbox,
            "recommended_actions": (
                ["Quarantine file", "Block hash", "Hunt for lateral movement"]
                if is_malicious
                else ["Continue monitoring"]
            ),
        }
